# Efficient Foundation Model Survey

This repo contains the paper list for survey ['Resource-efficient Algorithms and Systems for Large Foundation Models: A Survey'](https://arxiv.org).

References:

- https://github.com/MobileLLM/Personal_LLM_Agents_Survey

Metadata: 

- Authors: Mengwei Xu

- Affiliation: Beijing University of Posts and Telecommunications

- arXiv: https://arxiv.org/

## Filter by Categories

> [!TIP]
> We aggregate papers by their categories to make readers easily retrieve them.
> Give it a try!

- LLMs: [link](/llms)

- Multimodal: [link](/multimodal)

- Diffusion: [link](/diffusion)


## Table of Content

## Foundation Model Overview

- Attention is all you need. <ins>NeurIPS'17</ins>, [[Paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)] [[Code](https://www.github.com)]

- Impact of Tokenization on Language Models: An Analysis for Turkish. <ins>ACM TALLIP'22</ins>, [[Paper](https://arxiv.org/abs/2204.08832)]

- xxx. <ins>arXiv</ins>, [[Paper](https://github.com)] [[Code](https://github.com)]

- Attention is all you need. <ins>*arXiv'17*</ins>, [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) [[Code]](https://github.com/hyunwoongko/transformer)
- Bert: Pre-training of deep bidirectional transformers for language understanding. <ins>*arXiv'18*</ins>, [[Paper]](https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ) [[Code]](https://github.com/google-research/bert)
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. <ins>*arXiv'19*</ins>, [[Paper]](https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E) [[Code]](https://huggingface.co/docs/transformers/model_doc/distilbert)
- Roberta: A robustly optimized bert pretraining approach. <ins>*arXiv'19*</ins>, [[Paper]](https://arxiv.org/pdf/1907.11692.pdf%5C) [[Code]](https://github.com/pytorch/fairseq)
- Sentence-bert: Sentence embeddings using siamese bert-networks. <ins>*EMNLP'19*</ins>, [[Paper]](https://arxiv.org/pdf/1908.10084) [[Code]](https://github.com/UKPLab/sentence-transformers)
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. <ins>*ACL'19*</ins>, [[Paper]](https://arxiv.org/pdf/1910.13461) [[Code]](https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre#code)
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. <ins>*arXiv'19*</ins>, [[Paper]](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf) [[Code]](https://github.com/google-research/text-to-text-transfer-transformer)
- Improving language understanding by generative pre-training. [[URL]](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)
- Language Models are Unsupervised Multitask Learners. [[URL]](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)
- Language Models are Few-Shot Learners. <ins>*NeurIPS'20*</ins>, [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) [[Code]](https://github.com/openai/gpt-3)
- GLM: General Language Model Pretraining with Autoregressive Blank Infilling. <ins>*arXiv'21*</ins>, [[Paper]](https://arxiv.org/pdf/2103.10360) [[Code]](https://github.com/THUDM/GLM)
- Palm: Scaling language modeling with pathways. <ins>*JMLR'22*</ins>, [[Paper]](https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf) [[Code]](https://github.com/lucidrains/PaLM-pytorch)
- Training language models to follow instructions with human feedback. <ins>*NeurIPS'22*</ins>, [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)
- Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. <ins>*JMLR'22*</ins>, [[Paper]](https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf)
- Glam: Efficient scaling of language models with mixture-ofexperts. <ins>*ICML'22*</ins>, [[Paper]](https://proceedings.mlr.press/v162/du22c.html)
- wav2vec 2.0: A framework for self-supervised learning of speech representations. <ins>*NeurIPS'20*</ins>, [[Paper]](https://ar5iv.org/abs/2006.11477) [[Code]](https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/README.md)
- HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. <ins>*IEEE/ACM Transactions on Audio,  Speech, and Language Processing'21*</ins>, [[Paper]](https://ar5iv.org/abs/2106.07447) [[Code]](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert)
- Robust Speech Recognition via Large-Scale Weak Supervision. <ins>*ICML'23*</ins>, [[Paper]](https://proceedings.mlr.press/v202/radford23a.html)
- GPT-4 Technical Report. <ins>*arXiv'23*</ins>, [[Paper]](https://ar5iv.org/abs/2303.08774)
- Palm 2 technical report. [[URL]](https://ar5iv.org/abs/2305.10403)
- Llama 2: Open foundation and fine-tuned chat models. <ins>*arXiv'23*</ins>, [[Paper]](https://llama-2.ai/download/) [[Code]](https://github.com/facebookresearch/llama)


### Large Language Model

### Vision Transformer

### Multimodal

## Resource-efficient Architectures

## Resource-efficient Algorithms

## Resource-efficient Systems
